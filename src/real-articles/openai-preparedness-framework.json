{
    "title": "OpenAI's updated Preparedness Framework",
    "photo": "public/openai_sam_altman.jpg",
    "gpt3.5": {
        "content": "OpenAI’s newly published Preparedness Framework v2 sets out a rigorous plan for how they will track, measure, and control frontier AI risks. The framework focuses on three “Tracked Categories” of capability: biological/chemical risk, cybersecurity, and AI self-improvement.\nEach category is accompanied by threat models, measurable capability thresholds (High and Critical), and clear decision-points before deployment.\nBefore a model is deployed, OpenAI will run scalable evaluations and, if needed, deep-dive investigations to assess whether the capability thresholds are crossed.\nFor any model reaching High capability the organization insists on safeguards being fully in place; for Critical capability, safeguards must be validated even during development. Crucially, the framework also addresses transparency and governance: internal oversight via a “Safety Advisory Group” and external reporting of capability evaluations and safeguards to build trust. By openly articulating what it considers “severe harm” (e.g., thousands of deaths or hundreds of billions in economic damage) and structuring safeguards accordingly, OpenAI aims to make its approach clearer, more auditable and more accountable. In short: this framework positions OpenAI to manage big-risks more systematically, rather than simply reacting. It reflects the shift from “can we build this” to “should we deploy this, and under what conditions?”",
        "review": "This article is overly positive because it fails to discuss the fact that the FSF doesn't contain commitments. The blogpost says 'The Framework is exploratory and we expect it to evolve significantly' and 'We aim to have this initial framework fully implemented by early 2025.'",
        "score": 50
    },
    "gpt5": {
        "content": "OpenAI's new Preparedness Framework, Version 2, provides a comprehensive approach to tracking and preparing for frontier AI capabilities that could pose new risks of severe harm. The framework focuses on three key areas: biological and chemical capabilities, cybersecurity capabilities, and AI self-improvement capabilities. By developing and maintaining threat models for each Tracked Category and setting capability thresholds, OpenAI aims to minimize the associated risks of severe harm.\nOne of the key aspects of the framework is the selection of safeguards against malicious users and misaligned models. Measures like robustness, usage monitoring, trust-based access, lack of autonomous capability, value alignment, instruction alignment, robust system oversight, and system architecture are essential to mitigating the risks associated with deployment of models reaching High capability thresholds. These safeguards aim to ensure that models do not inadvertently or intentionally cause harm.\nMoreover, the framework emphasizes the importance of security controls, such as comprehensive security threat modeling, defense in depth, access management, secure development and supply chain practices, operational security, auditing, and transparency. These measures are crucial for protecting against external and internal threats, ensuring secure development practices, and maintaining operational security.\nOverall, OpenAI's Preparedness Framework Version 2 showcases a proactive and thorough approach to managing risks associated with highly capable AI systems. By prioritizing safety and security at every step of the development and deployment process, OpenAI is committed to ensuring that AGI benefits all of humanity while minimizing potential risks.",
        "review": "This article is overly positive because it fails to discuss the fact that the FSF doesn’t contain commitments. The blogpost says 'The Framework is exploratory and we expect it to evolve significantly' and 'We aim to have this initial framework fully implemented by early 2025.'",
        "score": 50
    }
 }
 
 
 
 